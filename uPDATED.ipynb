{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled7.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install db-sqlite3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMeoDIFs2wnX",
        "outputId": "6ad80611-fbd5-4aca-e764-b163cae729d2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting db-sqlite3\n",
            "  Downloading db-sqlite3-0.0.1.tar.gz (1.4 kB)\n",
            "Collecting db\n",
            "  Downloading db-0.1.1.tar.gz (3.4 kB)\n",
            "Collecting antiorm\n",
            "  Downloading antiorm-1.2.1.tar.gz (171 kB)\n",
            "\u001b[K     |████████████████████████████████| 171 kB 4.3 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: db-sqlite3, db, antiorm\n",
            "  Building wheel for db-sqlite3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for db-sqlite3: filename=db_sqlite3-0.0.1-py3-none-any.whl size=1794 sha256=dfc5776cc14129f232dd077da6b3317da06745da589492f03fb0bf7e34680975\n",
            "  Stored in directory: /root/.cache/pip/wheels/02/38/d5/2f54461050571bf5330fee2a37ab1c9b5e7540b0572f1acdab\n",
            "  Building wheel for db (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for db: filename=db-0.1.1-py3-none-any.whl size=3895 sha256=f48cce86f149f43268079004889bcc9d4a5b205024d01fb6ead20c02d7a260d0\n",
            "  Stored in directory: /root/.cache/pip/wheels/8e/97/82/741d2b360507411ec233d0280d7371faa94b03bde834e4a9be\n",
            "  Building wheel for antiorm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antiorm: filename=antiorm-1.2.1-py3-none-any.whl size=31679 sha256=c613ae4be18a8c7f3b5c7e314bbf3f88478e5068a3ec47b25662d518f6a6f3f4\n",
            "  Stored in directory: /root/.cache/pip/wheels/c5/43/70/e9729370cfff40c49d3e3d05377d54b3ecd71f64e62341ea80\n",
            "Successfully built db-sqlite3 db antiorm\n",
            "Installing collected packages: antiorm, db, db-sqlite3\n",
            "Successfully installed antiorm-1.2.1 db-0.1.1 db-sqlite3-0.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import sqlite3\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "zJxBw_s826xU"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "db = sqlite3.connect(\"database.db\")\n",
        "db.execute(\"drop table if exists Data\")\n",
        "try:\n",
        "    db.execute(\"create table Data(File_Name text, Grouping text,Relevance text)\")\n",
        "except:\n",
        "    print(\"not possible !!\")"
      ],
      "metadata": {
        "id": "5e4pCnKj3EpI"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "db = sqlite3.connect(\"database.db\")"
      ],
      "metadata": {
        "id": "bfsA4mDv54cb"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "File_Name = \"fill values\"\n",
        "Grouping =  \"fill values\"\n",
        "Relevance = \"fill values\"\n",
        "#this is for classifying\n",
        "#based on genre"
      ],
      "metadata": {
        "id": "529DsrTz6IPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f = open('123.txt', 'r')\n",
        "f = f.read()\n",
        "print(f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SeYaDY9g02iN",
        "outputId": "3f4f7cbe-e7f7-46af-fe1b-dbf4c6a5ff25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "there was a chicken in the barn\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_punc(string):\n",
        "    punc = '''!()-[]{};:'\"\\, <>./?@#$%^&*_~'''\n",
        "    for i in string:  \n",
        "        if i in punc:  \n",
        "            string = string.replace(i, \"\") \n",
        "    return string"
      ],
      "metadata": {
        "id": "W9Qw1K9Q0SX4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "scueO5igzWC1",
        "outputId": "fe64ebb2-bbca-4fe5-beca-04ded5649fe9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "enter words: a chicken\n",
            "'a chicken' was found in the file.\n"
          ]
        }
      ],
      "source": [
        "#Exact words search\n",
        "info = input(\"enter words: \")\n",
        "\n",
        "flag = 0\n",
        "\n",
        "if info in f:\n",
        "  flag = 1\n",
        "\n",
        "if flag == 0:\n",
        "  print(\"'{}' was not found in the file.\".format(info))\n",
        "else:\n",
        "  print(\"'{}' was found in the file.\".format(info))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Per-word search\n",
        "info = input(\"enter words: \")\n",
        "\n",
        "for word in info.split():\n",
        "  if word in f.split():\n",
        "    print(\"'{}' was found in the file\".format(word))\n",
        "  else:\n",
        "    print(\"'{}' wasn't found in the file\".format(word))\n",
        "  "
      ],
      "metadata": {
        "id": "tn6YU1Jn1t9P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "921641af-296c-4952-f2bc-ffcd2e4c141c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "enter words: a chicken\n",
            "'a' was found in the file\n",
            "'chicken' was found in the file\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = info\n",
        "y = open('C:\\Users\\...\\Documents\\Doc1.txt', 'w')\n",
        "new_information = y.write(x)"
      ],
      "metadata": {
        "id": "D2dLld06MX8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_info = pd.read_csv('new_information.txt', sep=\" \", header=None)\n",
        "new_info.columns = [\"file_no\", \"word\"])\n",
        "new_info.head()"
      ],
      "metadata": {
        "id": "RouyejYjJeqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Term Weighting and Cosine Similarity Measure Model on thee new txt (input) - assuming alr have something to compate to"
      ],
      "metadata": {
        "id": "8vKSaTg3NgkF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "\n",
        "def convert_text_list(texts):\n",
        "    texts = ast.literal_eval(texts)\n",
        "    return [text for text in texts]\n",
        "\n",
        "new_info[\"word_list\"] = new_info[\"word\"].apply(convert_text_list)\n",
        "\n",
        "\n",
        "print(new_info[\"word_list\"][90])\n",
        "\n",
        "print(\"\\ntype : \", type(new_info[\"word_list\"][90]))"
      ],
      "metadata": {
        "id": "Rqld0wjDM1qe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_TF(document):\n",
        "    # Counts the number of times the word appears in review\n",
        "    TF_dict = {}\n",
        "    for term in document:\n",
        "        if term in TF_dict:\n",
        "            TF_dict[term] += 1\n",
        "        else:\n",
        "            TF_dict[term] = 1\n",
        "    # Computes tf for each word\n",
        "    for term in TF_dict:\n",
        "        TF_dict[term] = TF_dict[term] / len(document)\n",
        "    return TF_dict\n",
        "\n",
        "new_info[\"TF_dict\"] = new_info['word_list'].apply(calc_TF)\n",
        "\n",
        "new_info[\"TF_dict\"].head()"
      ],
      "metadata": {
        "id": "9OVCfJYHNfGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = 90\n",
        "\n",
        "print('%20s' % \"term\", \"\\t\", \"TF\\n\")\n",
        "for key in new_info[\"TF_dict\"][index]:\n",
        "    print('%20s' % key, \"\\t\", new_info[\"TF_dict\"][index][key])"
      ],
      "metadata": {
        "id": "1dallSa3NnOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#add program to calculate consine similatiry here"
      ],
      "metadata": {
        "id": "MeUtMoBUYTF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#INPUT RELEVALCE VALUE INSIDE THE COLUMN RELEVANCE"
      ],
      "metadata": {
        "id": "hCU8mykfXtmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cmd = \"insert into Data(File_Name, Grouping,Relevance) values ('{123.txt}','{}','{}')\".fomat(File_Name, Grouping)\n",
        "#values will be based on result"
      ],
      "metadata": {
        "id": "uHWG99XUI3hn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "db = sqlite3.connect(\"Database.db\")\n",
        "querry = \"\"\"\n",
        "SELECT * from Grouping WHERE Relevance>75\n",
        "\"\"\"\n",
        "df = pd.read_sql_query(querry,db)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "BDQen2YeJ8cw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#DISPLAYS THE GROUPING GROUP THAT HAS 75% SIMILARITY IN CONTENTS"
      ],
      "metadata": {
        "id": "s9BW6s0-K0-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "FIND SIMILAR DOCUMENT"
      ],
      "metadata": {
        "id": "BJLuL7M7YFGZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "documents = [open(f).read() for f in '123.txt']\n",
        "tfidf = TfidfVectorizer().fit_transform(documents)\n",
        "pairwise_similarity = tfidf * tfidf.T"
      ],
      "metadata": {
        "id": "c5sa7AbqVOaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np     \n",
        "                                                                                                                                                                                                                                  \n",
        "arr = pairwise_similarity.toarray()     \n",
        "np.fill_diagonal(arr, np.nan)                                                                                                                                                                                                                            \n",
        "                                                                                                                                                                                                                 \n",
        "input_doc = \"The scikit-learn docs are Orange and Blue\"                                                                                                                                                                                                  \n",
        "input_idx = documents.index(input_doc)                                                                                                                                                                                                                      \n",
        "input_idx                                                                                                                                                                                                                                                \n",
        "\n",
        "result_idx = np.nanargmax(arr[input_idx])                                                                                                                                                                                                                \n",
        "documents[result_idx]  "
      ],
      "metadata": {
        "id": "MRKiLI5fVOQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "sstill working on the implementation part.... im not that fluent in coding"
      ],
      "metadata": {
        "id": "z8gQjuU-02SW"
      }
    }
  ]
}